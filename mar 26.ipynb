{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2727eb",
   "metadata": {},
   "source": [
    "ans1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f194452",
   "metadata": {},
   "source": [
    "simple linear regression have one feature and one target for example f1,y\n",
    "while on the other hand multiple linear regression have multiple features and one target f1,f2,,f3,f4,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0960c8ec",
   "metadata": {},
   "source": [
    "ans2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587bde8d",
   "metadata": {},
   "source": [
    "Linearity: The relationship between the independent variables and the dependent variable should be linear. That is, the change in the dependent variable should be proportional to the change in the independent variable.\n",
    "\n",
    "Independence: The observations in the dataset should be independent of each other. This means that the value of the dependent variable for one observation should not be influenced by the value of the dependent variable for another observation.\n",
    "\n",
    "Homoscedasticity: The variance of the errors should be constant across all levels of the independent variable(s). This means that the spread of the errors should be the same at all values of the independent variable(s).\n",
    "\n",
    "Normality: The errors should be normally distributed. This means that the distribution of the errors should be symmetric around zero.\n",
    "\n",
    "No multicollinearity: There should be no perfect linear relationship between the independent variables. This means that the independent variables should be independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22334028",
   "metadata": {},
   "source": [
    "ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf7bc4b",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope represents the change in the response variable (dependent variable) for a one-unit increase in the predictor variable (independent variable), assuming all other variables are held constant. The intercept represents the value of the response variable when the predictor variable is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7831c",
   "metadata": {},
   "source": [
    "ans4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4557d9db",
   "metadata": {},
   "source": [
    "Gradient descent is an iterative optimization algorithm used to find the minimum of a function, such as the cost function in machine learning models. The idea is to update the parameters of the model iteratively by moving in the direction of the steepest descent of the cost function.\n",
    "\n",
    "In machine learning, gradient descent is used to minimize the error between the predicted and actual values of the target variable. The goal is to find the best set of parameters (weights) that minimize the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49ca03",
   "metadata": {},
   "source": [
    "ans5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3717d40a",
   "metadata": {},
   "source": [
    "simple linear regression have one feature and one target for example f1,y\n",
    "while on the other hand multiple linear regression have multiple features and one target f1,f2,,f3,f4,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deca315",
   "metadata": {},
   "source": [
    "ans6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71cdb9e",
   "metadata": {},
   "source": [
    "Removing one of the correlated variables from the model\n",
    "Combining the correlated variables into a single variable (if they represent similar concepts)\n",
    "Using dimensionality reduction techniques, such as principal component analysis (PCA), to transform the correlated variables into a smaller set of uncorrelated variables\n",
    "Regularizing the regression coefficients using techniques such as Ridge regression or Lasso regression, which shrink the coefficients towards zero and can help reduce the impact of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cf8448",
   "metadata": {},
   "source": [
    "ans7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c733fc3f",
   "metadata": {},
   "source": [
    "The main difference between polynomial regression and linear regression is that in linear regression, the relationship between the independent and dependent variables is modeled as a straight line, while in polynomial regression, the relationship is modeled as a curved line that can be of any degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a0d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f536988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e40cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915bab0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a995aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
