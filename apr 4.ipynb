{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f82386d7",
   "metadata": {},
   "source": [
    "ans1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae7bbf4",
   "metadata": {},
   "source": [
    "A decision tree classifier is a type of supervised machine learning algorithm that is used for classification tasks. It works by creating a tree-like model of decisions and their possible consequences.\n",
    "\n",
    "The algorithm starts by examining the features of the training data and selecting the most important feature as the root node of the tree. It then splits the data into subsets based on the values of that feature, creating branches for each possible value. This process is repeated recursively for each subset of data, selecting the next most important feature to split on at each level, until the algorithm reaches a stopping criterion, such as a maximum depth of the tree or a minimum number of instances per leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520f6cc",
   "metadata": {},
   "source": [
    "ans2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d092f7f7",
   "metadata": {},
   "source": [
    "Calculate the impurity of the current set of instances: The impurity of a set of instances is a measure of how mixed the classes are in that set. One commonly used measure of impurity is the Gini impurity, which is defined as the sum of the squared probabilities of each class in the set:\n",
    "Gini impurity = 1 - Σ(p_i^2)\n",
    "\n",
    "where p_i is the probability of class i in the set.\n",
    "\n",
    "Calculate the information gain of each feature: The information gain of a feature measures how much it reduces the impurity of the set when used to split the data. The information gain of a feature can be calculated as follows:\n",
    "Information gain = impurity(parent) - Σ[(size(child)/size(parent)) * impurity(child)]\n",
    "\n",
    "where parent is the set of instances before the split, child is one of the subsets created by the split, and size() is the number of instances in a set.\n",
    "\n",
    "Choose the feature with the highest information gain: The feature with the highest information gain is chosen as the splitting criterion for the current node of the decision tree.\n",
    "\n",
    "Create a new branch for each value of the chosen feature: The data is split into subsets based on the values of the chosen feature, and a new branch is created for each subset.\n",
    "\n",
    "Recursively repeat steps 1-4 for each subset: The algorithm recursively applies the above steps to each subset until a stopping criterion is met, such as a maximum depth of the tree or a minimum number of instances per leaf.\n",
    "\n",
    "Assign a class label to each leaf node: Once the algorithm has finished building the decision tree, each leaf node is assigned a class label based on the majority class of the instances in that node.\n",
    "\n",
    "Predict the class label of a new instance: To predict the class label of a new instance, the decision tree algorithm traverses the tree from the root node to a leaf node, following the path that corresponds to the values of the input features. The class label assigned to the leaf node is then used as the predicted class for the input instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca39fa8",
   "metadata": {},
   "source": [
    "ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d2728",
   "metadata": {},
   "source": [
    "a decision tree classifier is a powerful and intuitive algorithm that can be used to solve a binary classification problem by recursively partitioning the feature space into regions that correspond to the different class labels, based on a set of decision rules that are learned from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b761282",
   "metadata": {},
   "source": [
    "ans4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbbc41",
   "metadata": {},
   "source": [
    "the geometric intuition behind decision tree classification is that it partitions the feature space into regions that correspond to different class labels, based on a set of decision rules. This geometric intuition can be used to make predictions on new instances by traversing the decision tree from the root node to a leaf node, and assigning the class label of the leaf node to the input instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79529774",
   "metadata": {},
   "source": [
    "ans5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c488941",
   "metadata": {},
   "source": [
    "Accuracy: The accuracy of the model is the proportion of instances that are correctly classified, which can be calculated as (TP + TN) / (TP + FP + TN + FN).\n",
    "\n",
    "Precision: The precision of the model is the proportion of instances predicted as positive that are actually positive, which can be calculated as TP / (TP + FP).\n",
    "\n",
    "Recall: The recall of the model is the proportion of actual positive instances that are correctly predicted as positive, which can be calculated as TP / (TP + FN).\n",
    "\n",
    "F1-score: The F1-score is the harmonic mean of precision and recall, which balances the tradeoff between precision and recall. It can be calculated as 2 * (precision * recall) / (precision + recall).\n",
    "\n",
    "Specificity: The specificity of the model is the proportion of actual negative instances that are correctly predicted as negative, which can be calculated as TN / (TN + FP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22476aeb",
   "metadata": {},
   "source": [
    "ans6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f290773d",
   "metadata": {},
   "source": [
    "         Predicted: Not Spam   | Predicted: Spam\n",
    "Actual: Not Spam\t  70\t   |   05\n",
    "Actual: Spam\t      10\t   |   15\n",
    "From this confusion matrix, we can calculate various evaluation metrics:\n",
    "\n",
    "Accuracy: The accuracy of the model can be calculated as (70 + 15) / (70 + 5 + 10 + 15) = 0.85, or 85%.\n",
    "\n",
    "Precision: The precision of the model for the spam class can be calculated as 15 / (5 + 15) = 0.75, or 75%. This means that out of all the instances the model predicted as spam, 75% of them were actually spam.\n",
    "\n",
    "Recall: The recall of the model for the spam class can be calculated as 15 / (10 + 15) = 0.6, or 60%. This means that out of all the actual spam instances, the model correctly predicted 60% of them as spam.\n",
    "\n",
    "F1-score: The F1-score is the harmonic mean of precision and recall, which balances the tradeoff between precision and recall. It can be calculated as 2 * (precision * recall) / (precision + recall) = 2 * (0.75 * 0.6) / (0.75 + 0.6) = 0.67."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e3f66",
   "metadata": {},
   "source": [
    "ans7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a9b7f",
   "metadata": {},
   "source": [
    "Choosing an appropriate evaluation metric for a classification problem is critical because it determines how the performance of the classification model will be measured. Different evaluation metrics may be appropriate for different types of classification problems, and choosing the wrong metric can lead to inaccurate or misleading conclusions about the model's performance.\n",
    "\n",
    "For example, in a binary classification problem where the positive class is rare, accuracy may not be a suitable evaluation metric because the model can achieve a high accuracy simply by predicting everything as the negative class. In such cases, precision or recall may be more appropriate evaluation metrics, as they focus on the performance of the model specifically on the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a69a7a",
   "metadata": {},
   "source": [
    "ans8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c89dfe",
   "metadata": {},
   "source": [
    "example of a classification problem where precision is the most important metric could be detecting fraudulent credit card transactions. In this problem, the positive class corresponds to fraudulent transactions, and the negative class corresponds to legitimate transactions.\n",
    "\n",
    "In this scenario, the cost of a false positive (classifying a legitimate transaction as fraudulent) can be very high. For example, if a legitimate transaction is blocked due to a false positive classification, the customer may be inconvenienced, and the reputation of the credit card company may be damaged. On the other hand, the cost of a false negative (classifying a fraudulent transaction as legitimate) may be lower, as the credit card company may be able to detect and handle the fraud at a later stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f8603",
   "metadata": {},
   "source": [
    "ans9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce0426",
   "metadata": {},
   "source": [
    "An example of a classification problem where recall is the most important metric is in detecting the presence of a rare disease, where early detection is critical for successful treatment. In this scenario, the positive class corresponds to individuals who have the disease, and the negative class corresponds to individuals who do not have the disease.\n",
    "\n",
    "In such a problem, the cost of a false negative (classifying an individual as negative when they actually have the disease) is very high, as it may lead to delayed diagnosis and treatment, and potentially fatal consequences. On the other hand, the cost of a false positive (classifying an individual as positive when they do not have the disease) is comparatively lower, as the individual can be retested and diagnosed correctly later on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
