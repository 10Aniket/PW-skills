{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067c7d04",
   "metadata": {},
   "source": [
    "ans1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd28491",
   "metadata": {},
   "source": [
    "Grid search CV (Cross-validation) is a hyperparameter tuning technique used in machine learning to find the best set of hyperparameters for a model that maximizes its performance on a validation set. Hyperparameters are model parameters that are set before training and can significantly affect the performance of a model, such as the learning rate, regularization strength, number of hidden layers, and activation functions.\n",
    "\n",
    "Define a grid of hyperparameters: Grid search involves defining a grid of hyperparameters that will be systematically searched to find the optimal combination. For example, if we're tuning the hyperparameters of a neural network, the grid might include the number of hidden layers, the number of neurons per layer, and the activation function to use for each layer.\n",
    "\n",
    "Split the data into training, validation, and test sets: The dataset is split into a training set, a validation set, and a test set. The training set is used to train the model, the validation set is used to evaluate the performance of the model during training, and the test set is used to evaluate the final performance of the model.\n",
    "\n",
    "Train and evaluate the model on each hyperparameter combination: For each combination of hyperparameters in the grid, a model is trained on the training set and evaluated on the validation set. The performance metric of interest, such as accuracy, F1 score, or AUC-ROC, is calculated for each combination of hyperparameters.\n",
    "\n",
    "Select the best hyperparameters: The hyperparameters that result in the best performance on the validation set are selected as the optimal set. The model is then trained on the combined training and validation sets using these hyperparameters and evaluated on the test set to estimate its performance on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226ec8d9",
   "metadata": {},
   "source": [
    "ans2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd1170",
   "metadata": {},
   "source": [
    "In grid search CV, the user specifies the range of hyperparameters to search over, and the algorithm evaluates all possible combinations of hyperparameters within that range. This approach can be effective for small hyperparameter spaces, where the optimal hyperparameters are likely to be near the center of the search space.\n",
    "\n",
    "In random search CV, the user defines a probability distribution over the hyperparameters, and the algorithm samples hyperparameters randomly from that distribution. This approach can be more efficient for large hyperparameter spaces, where the optimal hyperparameters may be far from the center of the search space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ece23",
   "metadata": {},
   "source": [
    "ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc13d54",
   "metadata": {},
   "source": [
    "Data leakage is a problem in machine learning where information from the training dataset is unintentionally included in the testing or validation dataset. This can lead to overly optimistic evaluation metrics and models that do not generalize well to new data.\n",
    "\n",
    "An example of data leakage is when training a model to predict the likelihood of a customer buying a product. If the training dataset includes information about whether the customer has already purchased the product, then the model will learn to use this information to make predictions, resulting in overly optimistic evaluation metrics. However, when the model is used to predict on new customers, it may not perform well as it has not learned to generalize to new customers who have not yet made a purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2394152b",
   "metadata": {},
   "source": [
    "ans4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa99a92a",
   "metadata": {},
   "source": [
    "reventing data leakage requires careful attention to the data and the features used in the model. It is important to be aware of the potential sources of data leakage and take steps to prevent it during the model building process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34cb2bf",
   "metadata": {},
   "source": [
    "ans5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446974f",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is used to evaluate the performance of a classification model by comparing the actual class labels to the predicted class labels. It is a matrix of four values that are used to calculate evaluation metrics for the model, such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "The four values in a confusion matrix are:\n",
    "\n",
    "True Positive (TP): The number of samples that were correctly predicted as positive.\n",
    "\n",
    "False Positive (FP): The number of samples that were incorrectly predicted as positive.\n",
    "\n",
    "True Negative (TN): The number of samples that were correctly predicted as negative.\n",
    "\n",
    "False Negative (FN): The number of samples that were incorrectly predicted as negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1702da28",
   "metadata": {},
   "source": [
    "ans6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7d16e",
   "metadata": {},
   "source": [
    "precision and recall are inversely related to each other, meaning that as one metric improves, the other tends to decrease. This is often referred to as the precision-recall trade-off. Therefore, it is important to find a balance between precision and recall that is appropriate for the specific problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8580b214",
   "metadata": {},
   "source": [
    "ans7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa39052",
   "metadata": {},
   "source": [
    "Accuracy: The overall proportion of correctly classified samples, which is calculated as (TP + TN) / (TP + TN + FP + FN). High accuracy indicates that the model is performing well, but it can be misleading if the dataset is imbalanced.\n",
    "\n",
    "Precision: The proportion of predicted positive samples that are actually positive, which is calculated as TP / (TP + FP). High precision indicates that the model is good at predicting positive samples correctly.\n",
    "\n",
    "Recall (Sensitivity): The proportion of actual positive samples that are correctly predicted as positive, which is calculated as TP / (TP + FN). High recall indicates that the model is good at identifying positive samples correctly.\n",
    "\n",
    "Specificity: The proportion of actual negative samples that are correctly predicted as negative, which is calculated as TN / (TN + FP). High specificity indicates that the model is good at identifying negative samples correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299d1590",
   "metadata": {},
   "source": [
    "ans8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1091d5",
   "metadata": {},
   "source": [
    "Some common metrics that can be derived from a confusion matrix include accuracy, precision, recall (or sensitivity), specificity, and F1 score. These metrics are calculated using the values in the confusion matrix, which shows the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) predicted by a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cdbbf7",
   "metadata": {},
   "source": [
    "ans9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06102090",
   "metadata": {},
   "source": [
    "The accuracy of a model is the proportion of correctly classified samples out of the total number of samples. It is calculated as (TP + TN) / (TP + TN + FP + FN). The values in the confusion matrix provide information on the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) predicted by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdef6d8",
   "metadata": {},
   "source": [
    "ans10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38734671",
   "metadata": {},
   "source": [
    "class imbalance: If the confusion matrix shows a large number of samples in one class compared to the other, it could indicate a class imbalance issue. This can be addressed by using techniques such as oversampling, undersampling, or using weighted loss functions.\n",
    "\n",
    "Biased predictions: If the model is predicting a particular class more often, it could indicate that the model is biased towards that class. This could be due to issues such as imbalanced data, feature selection, or hyperparameter tuning. In such cases, adjusting the data or fine-tuning the model can help overcome the bias.\n",
    "\n",
    "Overfitting: If the model has high accuracy on the training data but performs poorly on the test data, it could indicate that the model is overfitting. This can be identified by comparing the results of the confusion matrix on the training and test data. Regularization techniques can help prevent overfitting.\n",
    "\n",
    "Misclassification of specific samples: If the model is consistently misclassifying a specific type of sample, it could indicate a limitation of the model in identifying certain features or patterns. In such cases, feature engineering or modifying the model architecture can help improve performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
