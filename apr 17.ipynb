{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45128f85",
   "metadata": {},
   "source": [
    "ans1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31359f4e",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning algorithm that uses an ensemble of decision trees to make predictions about numerical data. It works by iteratively training decision trees on the residuals (differences between predicted and actual values) of the previous tree, and then combining the predictions of all the trees into a final prediction. The algorithm uses gradient descent to minimize the loss function (e.g., mean squared error) by adjusting the predictions of the decision trees in each iteration. Gradient Boosting Regression is known for its ability to handle complex, non-linear relationships between variables and to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea1a6a",
   "metadata": {},
   "source": [
    "ans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d07a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANIKET\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error:  18036.927697021172\n",
      "R-squared:  0.6982618224832821\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "        self.intercept = 0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # initialize residuals to be the target variable\n",
    "        residuals = y.copy()\n",
    "        \n",
    "        # loop through the number of estimators\n",
    "        for i in range(self.n_estimators):\n",
    "            # initialize a decision tree regressor\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            \n",
    "            # fit the tree to the training data and residuals\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # calculate the predictions of the tree\n",
    "            tree_preds = tree.predict(X)\n",
    "            \n",
    "            # update the residuals by subtracting the predictions of the tree\n",
    "            residuals -= self.learning_rate * tree_preds\n",
    "            \n",
    "            # append the tree to the list of estimators\n",
    "            self.estimators.append(tree)\n",
    "            \n",
    "        # calculate the intercept as the mean of the target variable\n",
    "        self.intercept = np.mean(y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # initialize the predictions as the intercept\n",
    "        preds = np.full(X.shape[0], self.intercept)\n",
    "        \n",
    "        # loop through the estimators and update the predictions\n",
    "        for tree in self.estimators:\n",
    "            preds += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "        return preds\n",
    "\n",
    "# create a small regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.5, random_state=42)\n",
    "\n",
    "# split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train the gradient boosting regressor on the training set\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "# evaluate the model using mean squared error and R-squared\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean squared error: \", mse)\n",
    "print(\"R-squared: \", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bd7c17",
   "metadata": {},
   "source": [
    "ans3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c721a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Generate a small regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, noise=0.5)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Define the parameter space to search\n",
    "param_dist = {\n",
    "    'learning_rate': np.linspace(0.01, 0.1, 10),\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "}\n",
    "\n",
    "# Define the model\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "# Perform random search to find the best hyperparameters\n",
    "rs = RandomizedSearchCV(gb, param_distributions=param_dist, n_iter=10, cv=5, scoring='neg_mean_squared_error')\n",
    "rs.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding MSE and R^2 on the test set\n",
    "print('Best hyperparameters:', rs.best_params_)\n",
    "y_pred = rs.predict(X_test)\n",
    "print('MSE on test set:', mean_squared_error(y_test, y_pred))\n",
    "print('R^2 on test set:', r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0836c473",
   "metadata": {},
   "source": [
    "ans4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bd2986",
   "metadata": {},
   "source": [
    "A weak learner in gradient boosting is a machine learning model that is only slightly better than random guessing on a classification or regression task. In the context of gradient boosting, a weak learner is typically a decision tree with a small depth or number of nodes.\n",
    "\n",
    "In gradient boosting, we iteratively add weak learners to the model to improve its performance. Each weak learner is trained on the residuals (the difference between the predicted and true values) of the previous model, with the goal of reducing the residual error. By combining multiple weak learners in this way, the model can learn complex patterns in the data and achieve high accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20eaed6",
   "metadata": {},
   "source": [
    "ans5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6fc56e",
   "metadata": {},
   "source": [
    "the intuition behind Gradient Boosting is to iteratively improve the model's predictions by adding new models that focus on the areas where the previous models performed poorly. This process continues until the algorithm reaches a specified number of iterations or the model's performance stops improving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204942da",
   "metadata": {},
   "source": [
    "ans6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b50935a",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners by adding them iteratively to the model, with each new weak learner correcting the errors made by the previous learners. This process continues until the desired number of weak learners has been added, or the model's performance has plateaued."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af6280",
   "metadata": {},
   "source": [
    "ans7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b49d37",
   "metadata": {},
   "source": [
    "Define a loss function that measures the difference between the predicted and true values of the target variable. The most common loss function used in Gradient Boosting is the mean squared error (MSE).\n",
    "\n",
    "Train a weak learner, such as a decision tree, on the original dataset to make predictions.\n",
    "\n",
    "Calculate the residuals, which are the differences between the predicted and true values of the target variable.\n",
    "\n",
    "Train another weak learner on the residuals to correct the errors made by the first learner. The goal is to fit a model that predicts the residuals as accurately as possible.\n",
    "\n",
    "Combine the predictions of the first and second learners to get an updated prediction. The updated prediction is closer to the true value than the prediction of the first learner alone.\n",
    "\n",
    "Repeat steps 3-5 for a specified number of iterations or until the model's performance plateaus.\n",
    "\n",
    "Combine the predictions of all the learners in a weighted manner to produce the final ensemble prediction. The weights are determined by the performance of each weak learner on a validation set, with better performing learners given higher weights.\n",
    "\n",
    "Regularize the model by adding penalties or constraints to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3dc1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d49824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa5638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53450359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba0182",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
