{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e15f3a",
   "metadata": {},
   "source": [
    "ans1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4eb8d9",
   "metadata": {},
   "source": [
    "R-squared, also known as the coefficient of determination, is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variable(s) in a linear regression model. It ranges from 0 to 1, where 0 indicates that the model explains none of the variance and 1 indicates that the model explains all of the variance.\n",
    "\n",
    "R-squared is calculated by taking the ratio of the explained variance to the total variance. The explained variance is the sum of the squared differences between the predicted values and the mean of the dependent variable, while the total variance is the sum of the squared differences between the actual values and the mean of the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d0b31d",
   "metadata": {},
   "source": [
    "ans2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0842b8ff",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified version of the R-squared metric used to evaluate the goodness of fit of a linear regression model. It represents the proportion of the variance in the dependent variable that is explained by the independent variables included in the model, adjusted for the number of predictors in the model.\n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R-squared)*(n - 1)/(n - k - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e855c593",
   "metadata": {},
   "source": [
    "ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c892d7eb",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate to use when comparing models with different numbers of predictors. This is because regular R-squared always increases as the number of predictors increases, even if the new predictors are not significantly improving the model. Adjusted R-squared, on the other hand, takes into account the number of predictors in the model and penalizes for adding irrelevant predictors that do not significantly improve the model's predictive power. Therefore, adjusted R-squared is a more accurate measure of the proportion of variance in the dependent variable that is explained by the model and can be used to compare the performance of models with different numbers of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c58422",
   "metadata": {},
   "source": [
    "ans4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c478fef1",
   "metadata": {},
   "source": [
    "\n",
    "In the context of regression analysis, RMSE (root mean squared error), MSE (mean squared error), and MAE (mean absolute error) are metrics used to evaluate the accuracy of a regression model's predictions.\n",
    "\n",
    "MSE represents the average of the squared differences between the predicted and actual values. It is calculated as follows:\n",
    "\n",
    "MSE = (1/n) * Σ(predicted value - actual value)^2\n",
    "\n",
    "RMSE is the square root of the MSE and is often used as a measure of the standard deviation of the residuals. It is calculated as follows:\n",
    "\n",
    "RMSE = √MSE\n",
    "\n",
    "MAE represents the average of the absolute differences between the predicted and actual values. It is calculated as follows:\n",
    "\n",
    "MAE = (1/n) * Σ|predicted value - actual value|\n",
    "\n",
    "In all three metrics, a lower value indicates better performance of the regression model in predicting the outcome variable. These metrics are commonly used to compare the performance of different regression models and to identify the best model for a given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95602cd7",
   "metadata": {},
   "source": [
    "ans5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10268eb0",
   "metadata": {},
   "source": [
    "while RMSE, MSE, and MAE are commonly used and provide useful information about the error of a model, they should be used in conjunction with other metrics and evaluation methods to fully assess the performance of a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc48ce6",
   "metadata": {},
   "source": [
    "ans6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727c39ab",
   "metadata": {},
   "source": [
    "Lasso regularization is more appropriate when there are many features in the data set, some of which are irrelevant or redundant, and sparsity is desirable. Ridge regularization may be more appropriate when all features are potentially relevant, and the coefficients should be shrunk towards zero without necessarily setting any of them exactly to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ee51e",
   "metadata": {},
   "source": [
    "ans7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91929b05",
   "metadata": {},
   "source": [
    "Regularized linear models help prevent overfitting in machine learning by adding a penalty term to the cost function that minimizes the sum of the squared error between the predicted and actual values. This penalty term helps to control the complexity of the model and reduces the influence of high-dimensional or irrelevant features on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace5f15f",
   "metadata": {},
   "source": [
    "ans8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e490a1d9",
   "metadata": {},
   "source": [
    "regularized linear models are a useful tool for regression analysis, but they have limitations. It is important to carefully consider the specific characteristics of the data before deciding whether to use regularized linear models or other modeling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec3849",
   "metadata": {},
   "source": [
    "ans9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c4189f",
   "metadata": {},
   "source": [
    "the choice of evaluation metric depends on the specific requirements of the problem and should be carefully considered. It is also important to consider the limitations of the metric and interpret the results in context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f846098d",
   "metadata": {},
   "source": [
    "ans10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d022da98",
   "metadata": {},
   "source": [
    "The choice between Ridge and Lasso regularization depends on the specific problem and the characteristics of the data. Generally, Ridge regularization is more appropriate when there are many predictors with small to moderate effect sizes, while Lasso regularization is more appropriate when there are a smaller number of predictors with large effect sizes and some degree of collinearity among them.\n",
    "\n",
    "In this case, we cannot make a direct comparison between Model A and Model B based solely on the regularization parameter and the type of regularization used. We would need to examine other factors, such as the performance metrics, the specific features included in each model, and the specific goals of the analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
