{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15cf8eeb",
   "metadata": {},
   "source": [
    "ans1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c505c7",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a simple, non-parametric machine learning algorithm used for both classification and regression tasks. It works by finding the K nearest neighbors to a query point in a dataset and predicting the label or value based on the majority class or average of the K neighbors. The distance between the query point and each data point is calculated using a distance metric such as Euclidean distance. The value of K, the number of neighbors, is a hyperparameter that can be tuned to optimize the model's performance. KNN is often used in real-world applications such as recommender systems, image recognition, and anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b1ff7",
   "metadata": {},
   "source": [
    "ans2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b069d92f",
   "metadata": {},
   "source": [
    "The value of K in the K-Nearest Neighbors (KNN) algorithm is a hyperparameter that can have a significant impact on the model's performance. Choosing the right value of K is important to prevent underfitting or overfitting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec702a",
   "metadata": {},
   "source": [
    "ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2bc925",
   "metadata": {},
   "source": [
    "In KNN classification, the goal is to predict a class label for a given input based on the majority class among its K nearest neighbors in the training data. The output of a KNN classifier is a categorical variable, representing the predicted class label. KNN classification is a type of supervised learning used for classification tasks.\n",
    "\n",
    "In contrast, KNN regression is used for continuous or numerical prediction tasks. Instead of predicting a class label, the goal is to predict a continuous value for a given input based on the average of the target variable among its K nearest neighbors in the training data. The output of a KNN regressor is a numerical variable, representing the predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afa6f74",
   "metadata": {},
   "source": [
    "ans4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b8822e",
   "metadata": {},
   "source": [
    "Accuracy: This measures the proportion of correctly classified samples among all the samples. It is a common metric used for classification tasks.\n",
    "\n",
    "Precision and recall: These metrics are used to evaluate the performance of a classification model, especially when the class distribution is imbalanced. Precision measures the proportion of correctly predicted positive samples among all the predicted positive samples, while recall measures the proportion of correctly predicted positive samples among all the actual positive samples.\n",
    "\n",
    "F1-score: This is a combination of precision and recall that provides a balanced measure of a model's performance.\n",
    "\n",
    "Mean squared error (MSE): This is a commonly used metric for regression tasks that measures the average squared difference between the predicted and actual values.\n",
    "\n",
    "R-squared (R2): This measures the proportion of variance in the target variable that is explained by the model. A value of 1 indicates a perfect fit, while a value of 0 indicates that the model does not explain any of the variance in the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00788617",
   "metadata": {},
   "source": [
    "ans5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5103ea92",
   "metadata": {},
   "source": [
    "The curse of dimensionality in KNN refers to the phenomenon where the performance of the KNN algorithm deteriorates as the number of features or dimensions in the dataset increases. As the number of dimensions increases, the volume of the feature space increases exponentially, which means that the number of training samples required to cover the space also increases exponentially. This makes it harder for the KNN algorithm to find the nearest neighbors, as the distance between the points becomes less meaningful in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03d9992",
   "metadata": {},
   "source": [
    "ans6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a789ba",
   "metadata": {},
   "source": [
    "Deletion: The simplest approach is to remove all instances that have missing values. However, this approach may result in loss of valuable data, especially if the dataset has a large number of missing values.\n",
    "\n",
    "Imputation: Another approach is to impute the missing values with a value based on the other available features in the dataset. Some common imputation techniques include mean imputation, median imputation, and mode imputation.\n",
    "\n",
    "KNN imputation: In this approach, the missing values are estimated using the KNN algorithm itself. The algorithm searches for the K nearest neighbors to the instance with missing values and uses their values to estimate the missing value.\n",
    "\n",
    "Weighted imputation: In this approach, the values of the nearest neighbors are weighted based on their proximity to the instance with missing values. The weights are then used to estimate the missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d867457e",
   "metadata": {},
   "source": [
    "ans7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55247b53",
   "metadata": {},
   "source": [
    "he performance of KNN classifier and regressor depends on the nature of the problem and the specific characteristics of the dataset. For classification problems, KNN classifier is typically the preferred choice, while for regression problems, KNN regressor is preferred. However, it's always a good practice to evaluate the performance of both algorithms on the specific dataset and choose the one that performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0068885f",
   "metadata": {},
   "source": [
    "ans8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a4bd6",
   "metadata": {},
   "source": [
    "**Strengths:\n",
    "\n",
    "KNN is a simple and easy-to-understand algorithm that can be applied to both classification and regression tasks.\n",
    "KNN can handle non-linear relationships between features and target variables.\n",
    "KNN does not make any assumptions about the underlying data distribution.\n",
    "KNN can be easily adapted for multi-class classification problems by using techniques such as one-vs-all or one-vs-one.\n",
    "\n",
    "**Weaknesses:\n",
    "\n",
    "KNN can be computationally expensive, especially for large datasets and high-dimensional feature spaces.\n",
    "KNN can be sensitive to the choice of distance metric and the value of K.\n",
    "KNN can perform poorly when the feature space is sparse or contains irrelevant features.\n",
    "KNN can be affected by the presence of outliers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaf4be4",
   "metadata": {},
   "source": [
    "ans9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce85c16",
   "metadata": {},
   "source": [
    "Euclidean distance is the shortest path between source and destination which is a straight line as shown in Figure 1.3. but Manhattan distance is sum of all the real distances between source(s) and destination(d) and each distance are always the straight lines as shown in Figure 1.4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41389c99",
   "metadata": {},
   "source": [
    "ans10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45baccb",
   "metadata": {},
   "source": [
    "Feature scaling is essential for machine learning algorithms that calculate distances between data. If not scaled, the feature with a higher value range starts dominating when calculating distances. KNN which uses Euclidean distance is one such algorithm which essentially require scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010bed3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a36755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc0995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
