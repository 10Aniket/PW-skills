{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b240d7bc",
   "metadata": {},
   "source": [
    "ans1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1308760e",
   "metadata": {},
   "source": [
    "Anomaly detection is the ability to identify rare items or observations that don't conform to normal or common patterns found in data. These outliers are important within financial data because they can indicate potential risks, control failures, or business opportunities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0140e927",
   "metadata": {},
   "source": [
    "ans2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14ea83",
   "metadata": {},
   "source": [
    "Challenges in anomaly detection include appropriate feature extraction, defining normal behaviors, handling imbalanced distribution of normal and abnormal data, addressing the variations in abnormal behavior, sparse occurrence of abnormal events, environmental variations, camera movements, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9823abd",
   "metadata": {},
   "source": [
    "ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fbce9e",
   "metadata": {},
   "source": [
    "unsupervised anomaly detection operates without prior knowledge of anomalies and models the normal behavior of the data to identify deviations. On the other hand, supervised anomaly detection relies on labeled data and builds a model based on the provided labels to differentiate between normal and anomalous instances. The choice between the two approaches depends on the availability of labeled data, the nature of anomalies, and the specific requirements of the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56ad33",
   "metadata": {},
   "source": [
    "ans4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77475f23",
   "metadata": {},
   "source": [
    "Statistical Methods: Statistical methods assume that the majority of the data follows a known statistical distribution. Anomalies are identified as instances that significantly deviate from the expected distribution. Common techniques include the use of probability density estimation, Gaussian distribution modeling, and z-score analysis.\n",
    "\n",
    "Machine Learning-Based Methods: Machine learning-based methods aim to learn patterns and relationships from labeled or unlabeled data to identify anomalies. These methods can be further classified into supervised and unsupervised approaches. Supervised methods use labeled data to train a model that can differentiate between normal and anomalous instances. Unsupervised methods detect anomalies based on deviations from the learned normal behavior of the data.\n",
    "\n",
    "Nearest Neighbor Methods: Nearest neighbor methods identify anomalies by measuring the distance or dissimilarity between data points. Anomalies are typically instances that have unusual distances from their neighboring points. Common techniques include k-nearest neighbors, local outlier factor (LOF), and density-based spatial clustering of applications with noise (DBSCAN).\n",
    "\n",
    "Clustering Methods: Clustering methods group data points into clusters based on their similarity. Anomalies are identified as instances that do not belong to any cluster or are distant from all clusters. Techniques such as k-means clustering, hierarchical clustering, and self-organizing maps (SOM) can be used for anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611cd3d",
   "metadata": {},
   "source": [
    "ans5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2518d7",
   "metadata": {},
   "source": [
    "Distance Metric: Distance-based methods assume that a meaningful distance metric exists to measure the dissimilarity between data points. This metric should capture the similarity or dissimilarity based on the characteristics of the data and the problem domain.\n",
    "\n",
    "Normal Data Distribution: These methods often assume that the majority of the data points follow a normal or expected distribution. Anomalies are considered as instances that deviate significantly from this expected distribution.\n",
    "\n",
    "Local Neighborhood: Distance-based methods focus on the local neighborhood of data points. They assume that normal instances are likely to be surrounded by similar instances within a certain distance. Anomalies, on the other hand, may be isolated or distant from the majority of the data points.\n",
    "\n",
    "Cluster Structure: Some distance-based methods assume the presence of distinct clusters in the data. They assume that anomalies either do not belong to any cluster or are far from the clusters. These methods rely on the assumption that normal instances form dense and cohesive clusters.\n",
    "\n",
    "Homogeneity of Data: Distance-based methods assume that the data is relatively homogeneous, meaning that the majority of the instances share similar characteristics. Anomalies are expected to exhibit different patterns or behaviors compared to the rest of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94469e6c",
   "metadata": {},
   "source": [
    "ans6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3722ce42",
   "metadata": {},
   "source": [
    "Local Reachability Density (LRD): LOF calculates the local reachability density (LRD) for each data point. The LRD of a point measures the density of its local neighborhood relative to the densities of its neighboring points. It quantifies how reachable a point is within its local region.\n",
    "\n",
    "k-Distance and k-Nearest Neighbors: LOF determines the k-distance and k-nearest neighbors for each data point. The k-distance of a point is the distance to its k-th nearest neighbor. The k-nearest neighbors of a point are the k data points closest to it.\n",
    "\n",
    "Local Reachability Distance (LRD of Neighbors): LOF computes the local reachability distance (LRD) for each neighbor of a data point. It is the inverse of the average reachability distance from the neighbor to its k-nearest neighbors.\n",
    "\n",
    "Local Outlier Factor (LOF): Finally, LOF calculates the local outlier factor (LOF) for each data point. It is the average ratio of the LRDs of a point's neighbors to its own LRD. The LOF value reflects how much the density of a point's local neighborhood differs from the densities of its neighbors. Higher LOF values indicate that a point is relatively less dense compared to its neighbors, suggesting it is more likely to be an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3102e3fc",
   "metadata": {},
   "source": [
    "ans7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e412d",
   "metadata": {},
   "source": [
    "Number of Trees (n_estimators): This parameter determines the number of isolation trees to be constructed in the forest. Increasing the number of trees generally improves the performance of the algorithm but also increases computation time.\n",
    "\n",
    "Subsample Size (max_samples): It specifies the number of samples to be used for constructing each isolation tree. Setting a smaller value can speed up the algorithm but may result in a less accurate estimation of anomalies.\n",
    "\n",
    "Maximum Tree Depth (max_depth): This parameter controls the maximum depth of each isolation tree. Deeper trees can better capture complex relationships in the data but may also increase the risk of overfitting.\n",
    "\n",
    "Contamination: The contamination parameter represents the expected proportion of anomalies in the dataset. It helps in adjusting the decision threshold for identifying anomalies. It is typically set based on prior knowledge or estimation of the anomaly rate in the dataset.\n",
    "\n",
    "Random Seed (random_state): This parameter allows for reproducibility of results by fixing the random seed used for generating random numbers in the algorithm. Setting a specific random seed ensures consistent results across multiple runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d2c10e",
   "metadata": {},
   "source": [
    "ans8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c18a6",
   "metadata": {},
   "source": [
    "In the K-nearest neighbors (KNN) algorithm, the anomaly score for a data point is typically computed based on the distance to its K nearest neighbors. In this case, if a data point has only 2 neighbors of the same class within a radius of 0.5, and we are using K=10 in the KNN algorithm, the anomaly score for that data point will be relatively high.\n",
    "\n",
    "Since the data point has only 2 neighbors within the specified radius, it suggests that the data point is not well-represented in its local neighborhood. In the context of anomaly detection, this can be an indication that the data point is significantly different from its neighboring points and may be considered as an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd9928b",
   "metadata": {},
   "source": [
    "ans9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accff2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
