{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f2c0ad",
   "metadata": {},
   "source": [
    "ans1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23312de1",
   "metadata": {},
   "source": [
    "Ensemble methods are techniques that aim at improving the accuracy of results in models by combining multiple models instead of using a single model. The combined models increase the accuracy of the results significantly. This has boosted the popularity of ensemble methods in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c2f62",
   "metadata": {},
   "source": [
    "ans2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99fe35b",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several reasons:\n",
    "\n",
    "Improved prediction accuracy: Ensemble methods combine the predictions of multiple models, which can lead to better overall prediction accuracy. By combining the predictions of multiple models, the ensemble can mitigate the weaknesses of any individual model and capture the strengths of different models.\n",
    "\n",
    "Robustness: Ensemble techniques can be more robust than individual models because they are less sensitive to noisy or irrelevant features. If a single model is trained on a noisy dataset or with irrelevant features, its predictions may be inaccurate. However, by combining the predictions of multiple models, the ensemble can reduce the impact of such noise and improve prediction accuracy.\n",
    "\n",
    "Overfitting reduction: Ensemble methods can also reduce overfitting, which occurs when a model is trained to fit the training data too closely and therefore does not generalize well to new data. By combining the predictions of multiple models, the ensemble can reduce the risk of overfitting and improve the model's ability to generalize to new data.\n",
    "\n",
    "Handling high-dimensional data: Ensemble techniques can handle high-dimensional data more effectively than individual models. This is because the ensemble can leverage different models to identify relevant features and reduce the impact of irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58632887",
   "metadata": {},
   "source": [
    "ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6b16f5",
   "metadata": {},
   "source": [
    "Bagging, short for Bootstrap Aggregating, is an ensemble technique in machine learning that involves training multiple instances of the same model on different subsets of the training data, and then combining their predictions to produce a final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c467ed9d",
   "metadata": {},
   "source": [
    "ans4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe2207e",
   "metadata": {},
   "source": [
    "Boosting is an ensemble technique in machine learning that combines multiple weak learners into a strong learner by training them sequentially, with each subsequent model attempting to correct the errors of the previous model. Unlike bagging, which trains multiple models independently and aggregates their predictions, boosting trains a series of models in a sequential manner and adapts each model to the errors of the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a35e1f",
   "metadata": {},
   "source": [
    "ans5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8244202",
   "metadata": {},
   "source": [
    "Improved prediction accuracy: Ensemble techniques can improve the accuracy of predictions by combining the strengths of multiple models and mitigating their weaknesses. By combining the predictions of multiple models, ensemble methods can provide more accurate and robust predictions than any individual model.\n",
    "\n",
    "Robustness: Ensemble methods are more robust than individual models because they are less sensitive to noisy or irrelevant features in the data. If a single model is trained on a noisy dataset or with irrelevant features, its predictions may be inaccurate. However, by combining the predictions of multiple models, ensemble techniques can reduce the impact of such noise and improve prediction accuracy.\n",
    "\n",
    "Overfitting reduction: Ensemble techniques can reduce overfitting, which occurs when a model is trained to fit the training data too closely and therefore does not generalize well to new data. By combining the predictions of multiple models, ensemble techniques can reduce the risk of overfitting and improve the model's ability to generalize to new data.\n",
    "\n",
    "Handling high-dimensional data: Ensemble techniques can handle high-dimensional data more effectively than individual models. This is because the ensemble can leverage different models to identify relevant features and reduce the impact of irrelevant features.\n",
    "\n",
    "Flexibility: Ensemble techniques are flexible and can be applied to a wide range of machine learning problems, including classification, regression, and clustering.\n",
    "\n",
    "Overall, ensemble techniques are a powerful tool for improving the accuracy, robustness, and generalization ability of machine learning models, and are widely used in practice to achieve state-of-the-art results in various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3187751",
   "metadata": {},
   "source": [
    "ans6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585ec6da",
   "metadata": {},
   "source": [
    "ensemble techniques are a powerful tool in machine learning and can improve the accuracy, robustness, and generalization ability of models in many cases. However, their effectiveness depends on several factors, and they may not always outperform individual models. It is important to evaluate the performance of both individual models and ensemble techniques and select the best approach based on the specific requirements of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd71589",
   "metadata": {},
   "source": [
    "ans7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa374a",
   "metadata": {},
   "source": [
    "Resample the original dataset with replacement to generate a large number of bootstrap samples, each of the same size as the original dataset.\n",
    "\n",
    "Compute the statistic of interest, such as the mean or median, for each bootstrap sample.\n",
    "\n",
    "Calculate the standard error of the statistic as the standard deviation of the bootstrap sample statistics.\n",
    "\n",
    "Calculate the desired level of confidence, such as 95% or 99%.\n",
    "\n",
    "Determine the number of bootstrap sample statistics that fall within the confidence interval using the desired level of confidence. For example, if the desired level of confidence is 95%, and there are 10,000 bootstrap samples, then the confidence interval will include the central 95% of the bootstrap sample statistics, which is equivalent to the 2.5th percentile to the 97.5th percentile of the bootstrap sample statistics.\n",
    "\n",
    "Determine the lower and upper bounds of the confidence interval by selecting the appropriate percentile values from the bootstrap sample statistics. For example, if the 2.5th percentile and the 97.5th percentile of the bootstrap sample statistics are 4.0 and 10.0, respectively, then the 95% confidence interval is [4.0, 10.0]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f4e04",
   "metadata": {},
   "source": [
    "ans8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27835c4e",
   "metadata": {},
   "source": [
    "Select a sample of size n from the population of interest.\n",
    "\n",
    "Resample the original sample with replacement to create a new sample of size n. This new sample is called a bootstrap sample.\n",
    "\n",
    "Calculate the statistic of interest, such as the mean or standard deviation, from the bootstrap sample.\n",
    "\n",
    "Repeat steps 2 and 3 a large number of times, typically 1,000 or more, to create a large number of bootstrap samples and estimate the distribution of the statistic.\n",
    "\n",
    "Calculate the standard error of the statistic as the standard deviation of the bootstrap sample statistics.\n",
    "\n",
    "Use the bootstrap distribution to calculate confidence intervals or conduct hypothesis tests for the population parameter of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6fec6f",
   "metadata": {},
   "source": [
    "ans9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6242c9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated population mean height: 14.455798937537757\n",
      "Standard error of the mean: 0.23800238625263176\n",
      "95% confidence interval: 13.976603161574634 - 14.91310539450129\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the original sample\n",
    "sample = np.random.normal(loc=15, scale=2, size=50)\n",
    "\n",
    "# Define the number of bootstrap samples to generate\n",
    "n_bootstraps = 10000\n",
    "\n",
    "# Generate the bootstrap samples and calculate the mean height for each sample\n",
    "bootstrap_means = []\n",
    "for i in range(n_bootstraps):\n",
    "    bootstrap_sample = np.random.choice(sample, size=len(sample), replace=True)\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "# Calculate the estimated population mean and standard error\n",
    "pop_mean = np.mean(bootstrap_means)\n",
    "pop_std_err = np.std(bootstrap_means, ddof=1)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "ci_lower = np.percentile(bootstrap_means, 2.5)\n",
    "ci_upper = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "print(\"Estimated population mean height:\", pop_mean)\n",
    "print(\"Standard error of the mean:\", pop_std_err)\n",
    "print(\"95% confidence interval:\", ci_lower, \"-\", ci_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e1cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86550ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51c2c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff5d1b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96a84e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
