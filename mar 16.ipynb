{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d256331",
   "metadata": {},
   "source": [
    "ans1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67151121",
   "metadata": {},
   "source": [
    "Overfitting occurs when a model is too complex and learns the noise and randomness in the training data, rather than the underlying patterns and relationships. This leads to a model that performs very well on the training data but poorly on new, unseen data. The consequences of overfitting are that the model's performance on new data may be poor, and it may be less generalizable.\n",
    "\n",
    "Underfitting, on the other hand, occurs when a model is too simple and fails to capture the underlying patterns and relationships in the data. This leads to a model that performs poorly on both the training data and new, unseen data. The consequences of underfitting are that the model may not be able to learn the important features and relationships in the data, and its predictive power may be limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f97636",
   "metadata": {},
   "source": [
    "ans2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6d2142",
   "metadata": {},
   "source": [
    "To reduce overfitting, there are several techniques that can be employed:\n",
    "\n",
    "Cross-validation: Use cross-validation to estimate the model's performance on new, unseen data. Cross-validation involves splitting the data into training and validation sets and training the model on the training set while evaluating its performance on the validation set. By doing this multiple times with different splits of the data, we can estimate the model's performance on new, unseen data.\n",
    "\n",
    "Regularization: Regularization is a technique that adds a penalty term to the loss function to discourage the model from having very large weights. This helps to simplify the model and reduce overfitting. Examples of regularization techniques include L1 and L2 regularization.\n",
    "\n",
    "Dropout: Dropout is a technique where we randomly drop out some neurons during training. This helps to prevent the model from relying too heavily on any one feature and encourages it to learn more robust features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c27eb76",
   "metadata": {},
   "source": [
    "ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e362b1",
   "metadata": {},
   "source": [
    "underfitting means the error is high on train data and error is high on test data aswell in this senario need to create a new model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d9cf6",
   "metadata": {},
   "source": [
    "ans4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aedacec",
   "metadata": {},
   "source": [
    "bias variance trade off affect model \n",
    "\n",
    "underfitting model : high bias , high variance \n",
    "\n",
    "overfitting model : low bias , high variance\n",
    "\n",
    "genralized model : low bias , low variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28af475",
   "metadata": {},
   "source": [
    "ans5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8343effa",
   "metadata": {},
   "source": [
    "Overfitting and underfitting are common problems in machine learning, and detecting them is crucial to building accurate models. Here are some common methods for detecting overfitting and underfitting:\n",
    "\n",
    "Training and validation loss: One way to detect overfitting and underfitting is by monitoring the training and validation loss during training. If the training loss continues to decrease while the validation loss starts to increase, this indicates that the model is overfitting. If both the training and validation loss are high, the model is underfitting.\n",
    "\n",
    "Learning curves: Learning curves show the performance of the model on the training and validation sets as a function of the number of training examples. If the training and validation curves converge, this indicates that the model is not overfitting or underfitting. If the validation curve is much higher than the training curve, this indicates that the model is overfitting.\n",
    "\n",
    "Evaluation on a hold-out set: One way to detect overfitting is by evaluating the model on a hold-out set that was not used for training. If the model performs poorly on the hold-out set, this indicates that the model is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e990431",
   "metadata": {},
   "source": [
    "ans6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f37192",
   "metadata": {},
   "source": [
    "Bias and variance are two types of errors that are common in machine learning models. Bias refers to the errors that arise due to the assumptions made by the model in order to simplify the learning process. High bias models tend to underfit the data, i.e., they are not able to capture the underlying patterns and relationships in the data. On the other hand, variance refers to the errors that arise due to the complexity of the model. High variance models tend to overfit the data, i.e., they fit the training data very well but perform poorly on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f8381",
   "metadata": {},
   "source": [
    "ans7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad62628",
   "metadata": {},
   "source": [
    "L1 regularization works by adding a penalty term to the loss function proportional to the absolute value of the weights. This penalty term forces some of the weights to become zero, effectively performing feature selection and reducing the complexity of the model.\n",
    "\n",
    "L2 regularization works by adding a penalty term to the loss function proportional to the square of the weights. This penalty term forces the weights to become small, effectively reducing the impact of any one feature and reducing the complexity of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
