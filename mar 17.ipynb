{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f15a273",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ans1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12512/271731697.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mans1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ans1' is not defined"
     ]
    }
   ],
   "source": [
    "ans1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc46db95",
   "metadata": {},
   "source": [
    "missing values in dataset is the value which have null values and no signifinance so we need to fill it with mean,meadian or mode and the alogrythm which are robust to null valeus randomforest, gradientdescent, knn etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529234bc",
   "metadata": {},
   "source": [
    "ans2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3263d80a",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "df = pd.read_csv({'sr':[1,2,3,4,5],'name':['jon','bran','arya','snasa', np.null]})\n",
    "\n",
    "df.dropna()#inplace=True \n",
    "df.dropna(axis=1, how='any') \n",
    "df.fillna(df.mean())\n",
    "df.fillna(method='ffill')\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_filled = pd.DataFrame(imputer.fit_transform(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f06891",
   "metadata": {},
   "source": [
    "ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0368b1",
   "metadata": {},
   "source": [
    "when the calssification target column is not dusribute in equal ratio and the conciquences of the imbalaced dataset are \n",
    "for example if the dataset is imbalanced then the model will trian it self on just on type of data if the problem statement is \n",
    "binary 0 and 1 then the model will train itself on just 0 and it will lead to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67931c42",
   "metadata": {},
   "source": [
    "ans4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e0f35a",
   "metadata": {},
   "source": [
    "Up-Sampling is a \"Zero-Padding Procedure\" that increase the number of samples of a DT signal. More specificals, when up sampling, zeros are added between the samples of a signal. Down-Sampling is to decrease the sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f86a6a",
   "metadata": {},
   "source": [
    "ans5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed503025",
   "metadata": {},
   "source": [
    "SMOTE is an algorithm that performs data augmentation by creating synthetic data points based on the original data points. SMOTE can be seen as an advanced version of oversampling, or as a specific algorithm for data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6fd462",
   "metadata": {},
   "source": [
    "ans6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917648e1",
   "metadata": {},
   "source": [
    "Outliers are the observations in a dataset that deviate significantly from the rest of the data. In any data science project, it is essential to identify and handle outliers, as they can have a significant impact on many statistical methods, such as means, standard deviations, etc., and the performance of ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cdc6e3",
   "metadata": {},
   "source": [
    "ans7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbee62d5",
   "metadata": {},
   "source": [
    "Deletion: In this technique, we remove the observations or variables with missing data. This technique is appropriate if the amount of missing data is small and does not significantly affect the overall analysis.\n",
    "\n",
    "Imputation: In this technique, we fill in the missing values with estimated values. There are several methods of imputation, including mean imputation, median imputation, mode imputation, and regression imputation. This technique is appropriate if the amount of missing data is moderate and the missing data is missing at random (i.e., there is no systematic reason why the data is missing).\n",
    "\n",
    "Prediction: In this technique, we use machine learning algorithms to predict the missing values. This technique is appropriate if the amount of missing data is large and the missing data is not missing at random (i.e., there is a systematic reason why the data is missing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7c935d",
   "metadata": {},
   "source": [
    "ans8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c476b3",
   "metadata": {},
   "source": [
    "Visualizations: One strategy is to create visualizations that show the missing data patterns. For example, we can create a heatmap that shows the locations of missing values. If the missing values are randomly distributed, we would expect to see a uniform distribution across the dataset. If there is a pattern to the missing values, we would expect to see a non-uniform distribution across the dataset.\n",
    "\n",
    "Correlation analysis: Another strategy is to perform correlation analysis between the missing values and other variables in the dataset. If the missing values are not correlated with any other variables, then they are likely missing at random. However, if there is a correlation between the missing values and other variables, then there may be a pattern to the missing data.\n",
    "\n",
    "Statistical tests: Another strategy is to use statistical tests to determine if the missing data is missing at random. One common test is the Little's MCAR test, which tests whether the missing data is completely at random. If the test fails, it suggests that there is a pattern to the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663abeeb",
   "metadata": {},
   "source": [
    "ans9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344858cf",
   "metadata": {},
   "source": [
    "Confusion matrix: A confusion matrix is a table that summarizes the performance of a classification model. It shows the number of true positives, true negatives, false positives, and false negatives. By looking at the confusion matrix, we can calculate metrics such as accuracy, precision, recall, and F1 score, which can provide insights into the performance of the model.\n",
    "\n",
    "ROC curve: The ROC (Receiver Operating Characteristic) curve is a graphical representation of the performance of a classification model at different thresholds. It plots the true positive rate (sensitivity) against the false positive rate (1-specificity). A good classifier will have a ROC curve that is close to the upper left corner of the plot, indicating high sensitivity and specificity.\n",
    "\n",
    "Precision-recall curve: The precision-recall curve is another graphical representation of the performance of a classification model. It plots precision (positive predictive value) against recall (sensitivity) at different thresholds. A good classifier will have a precision-recall curve that is close to the upper right corner of the plot, indicating high precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5086bae",
   "metadata": {},
   "source": [
    "ans10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c1c39",
   "metadata": {},
   "source": [
    "Random undersampling: This method involves randomly selecting a subset of the majority class samples and removing them from the dataset. This can be done until the number of majority class samples is equal to the number of minority class samples. Random undersampling can be applied using various techniques, such as RandomUnderSampler from the imbalanced-learn library in Python.\n",
    "\n",
    "Cluster centroid undersampling: This method involves clustering the majority class samples and selecting the centroid of each cluster as the representative sample for the cluster. These representative samples can be used to create a more balanced dataset. This can be done using the ClusterCentroids class in the imbalanced-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de6945e",
   "metadata": {},
   "source": [
    "ans11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb6b06",
   "metadata": {},
   "source": [
    "Random oversampling: This method involves randomly duplicating minority class samples to create a more balanced dataset. This can be done until the number of minority class samples is equal to the number of majority class samples. Random oversampling can be applied using various techniques, such as RandomOverSampler from the imbalanced-learn library in Python.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique): This method involves creating synthetic samples for the minority class by interpolating between existing samples. This can be done using the SMOTE class in the imbalanced-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9548239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
